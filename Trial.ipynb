{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf63e826-8b88-48e2-8e87-7488e024d92c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Imports\n",
    "\n",
    "from dh_segment_torch.config import Params\n",
    "from dh_segment_torch.data import DataSplitter\n",
    "from dh_segment_torch.data.annotation import AnnotationWriter\n",
    "from dh_segment_torch.training import Trainer\n",
    "from dh_segment_torch.inference import PredictProcess\n",
    "from dh_segment_torch.post_processing import PostProcessingPipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d0840303-3e1b-44a3-babc-ad32f9ca401e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image, ImageDraw, ImageColor\n",
    "import os, glob, json, cv2, collections, torch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f82e3c2a-8350-45a8-9e14-7baae89760ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\boiss\\anaconda3\\envs\\dhs\\lib\\site-packages\\torch\\cuda\\__init__.py:125: UserWarning: \n",
      "NVIDIA GeForce RTX 3050 Laptop GPU with CUDA capability sm_86 is not compatible with the current PyTorch installation.\n",
      "The current PyTorch install supports CUDA capabilities sm_37 sm_50 sm_60 sm_61 sm_70 sm_75 compute_37.\n",
      "If you want to use the NVIDIA GeForce RTX 3050 Laptop GPU GPU with PyTorch, please check the instructions at https://pytorch.org/get-started/locally/\n",
      "\n",
      "  warnings.warn(incompatible_device_warn.format(device_name, capability, \" \".join(arch_list), device_name))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA GeForce RTX 3050 Laptop GPU\n",
      "1.6.0+cu101\n",
      "10.1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(torch.cuda.get_device_name())\n",
    "print(torch.__version__)\n",
    "print(torch.version.cuda)\n",
    "x = torch.randn(1).cuda()\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa6024ab-d458-439e-9037-7f766880cfb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Loading data\n",
    "\n",
    "folder_name = 'lib/demo' # Change this parameter to your project name\n",
    "\n",
    "params = {\n",
    "    'data_path' : '{}\\data'.format(folder_name), # Path to write the data\n",
    "    'data_splitter': {'train_ratio': 0.8, 'val_ratio': 0.2, 'test_ratio': 0.0}, # splitting ratio of the data\n",
    "    'copy_images': True, # Whether to copy the images\n",
    "    'overwrite': True, # Whether to overwrite the images\n",
    "    'progress': True # Whether to show progress\n",
    "}\n",
    "\n",
    "data_path = params.pop(\"data_path\")\n",
    "\n",
    "color_label = {\n",
    "    'path': os.path.join(data_path, \"color_labels.json\"),\n",
    "    'colors': {\n",
    "        'background': [0, 0, 0], # RGB\n",
    "        'cardboard': [255, 0, 0], # RGB\n",
    "        'picture': [0, 0, 255] # RGB\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce66e279-7908-4e1b-aa7d-5aacd2cb2dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Process parameters\n",
    "\n",
    "num_processes = params.pop(\"num_processes\", 4)\n",
    "\n",
    "relative_path = params.pop(\"relative_path\", True)\n",
    "\n",
    "params.setdefault(\"labels_dir\", os.path.join(data_path, \"labels\"))\n",
    "labels_dir = params.get(\"labels_dir\")\n",
    "\n",
    "params.setdefault(\"images_dir\", os.path.join(data_path, \"images\"))\n",
    "images_dir = params.get(\"images_dir\")\n",
    "\n",
    "params.setdefault(\"color_labels_file_path\", os.path.join(data_path, \"color_labels.json\"))\n",
    "params.setdefault(\"csv_path\", os.path.join(data_path, \"data.csv\"))\n",
    "\n",
    "data_splitter_params = params.pop(\"data_splitter\", None)\n",
    "train_csv_path = params.pop(\"train_csv\", os.path.join(data_path, \"train.csv\"))\n",
    "val_csv_path = params.pop(\"val_csv\", os.path.join(data_path, \"val.csv\"))\n",
    "test_csv_path = params.pop(\"test_csv\", os.path.join(data_path, \"test.csv\"))\n",
    "\n",
    "params.setdefault(\"type\", \"image\")\n",
    "\n",
    "labels_list = sorted(glob.glob(os.path.join(labels_dir, '*.*')))\n",
    "images_list = sorted(glob.glob(os.path.join(images_dir, '*.*')))\n",
    "\n",
    "data = pd.DataFrame({'image': images_list, 'label': labels_list})\n",
    "data.to_csv(params['csv_path'], header=False, index=False)\n",
    "\n",
    "if relative_path:\n",
    "    data['image'] = data['image'].apply(lambda path: os.path.join(\"images\", os.path.basename(path)))\n",
    "    data['label'] = data['label'].apply(lambda path: os.path.join(\"labels\", os.path.basename(path)))\n",
    "    \n",
    "if data_splitter_params:\n",
    "    data_splitter = DataSplitter.from_params(data_splitter_params)\n",
    "    data_splitter.split_data(data, train_csv_path, val_csv_path, test_csv_path)\n",
    "    \n",
    "for class_name in color_label['colors'].keys():\n",
    "    if type(color_label['colors'][class_name]) == str:\n",
    "        color_label['colors'][class_name] = list(ImageColor.getcolor(\n",
    "            color_label['colors'][class_name], \"RGB\"))\n",
    "\n",
    "with open(color_label['path'], 'w') as outfile:\n",
    "    json.dump({'colors': list(color_label['colors'].values()),\n",
    "              'one_hot_encoding': None,\n",
    "              'labels': list(color_label['colors'].keys())}, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81541260-05a1-4a9f-973b-abb0526666e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Training params\n",
    "\n",
    "params = {\n",
    "    \"color_labels\": {\"label_json_file\": os.path.join(data_path, \"color_labels.json\")}, # Color labels produced before\n",
    "    \"train_dataset\": {\n",
    "        \"type\": \"image_csv\", # Image csv dataset\n",
    "        \"csv_filename\":  os.path.join(data_path, \"train.csv\"),\n",
    "        \"base_dir\": data_path,\n",
    "        \"repeat_dataset\": 1,\n",
    "        \"compose\": {\"transforms\": [{\"type\": \"random_shadow\", \"p\": 0.2},\n",
    "                                   {\"type\": \"vertical_flip\", \"p\": 0.3},\n",
    "                                   {\"type\": \"blur\", \"p\": 0.2, \"blur_limit\": 3}]}\n",
    "    },\n",
    "    \"val_dataset\": {\n",
    "        \"type\": \"image_csv\", # Validation dataset\n",
    "        \"csv_filename\": os.path.join(data_path, \"val.csv\"),\n",
    "        \"base_dir\": data_path,\n",
    "        \"compose\": {\"transforms\": []}\n",
    "    },\n",
    "    \"model\": { # Model definition, original dhSegment\n",
    "        \"encoder\": \"resnet101\", \n",
    "        \"decoder\": {\n",
    "            \"decoder_channels\": [512, 256, 128, 64, 32],\n",
    "            \"max_channels\": 512\n",
    "        }\n",
    "    },\n",
    "    \"initializer\": {\n",
    "        \"initializers\": [\n",
    "            { \"regexes\": \"decoder.*.conv2d.weight$\", \"type\": \"xavier_uniform\" },\n",
    "            { \"regexes\": \"decoder.*.conv2d.bias$\", \"type\": \"zeros\" }]\n",
    "    },\n",
    "    \"metrics\": [['miou', 'iou'], ['iou', {\"type\": 'iou', \"average\": None}], 'precision'], # Metrics to compute\n",
    "    \"optimizer\": {\"lr\": 5e-5}, # Learning rate\n",
    "    \"lr_scheduler\": {\"type\": \"exponential\", \"gamma\": 0.9995},\n",
    "    \"val_metric\": \"+miou\", # Metric to observe to consider a model better than another, the + indicates that we want to maximize\n",
    "    \"early_stopping\": { \"patience\": 25}, # Number of validation steps without increase to tolerate, stops if reached\n",
    "    \"model_out_dir\": f\"../models/{folder_name}\", # Path to model output\n",
    "    \"num_epochs\": 100, # Number of epochs for training\n",
    "    \"evaluate_every_epoch\": 1, # Number of epochs between each validation of the model\n",
    "    \"batch_size\": 1, # Batch size (to be changed if the allocated GPU has little memory)\n",
    "    \"num_data_workers\": 0,\n",
    "    \"track_train_metrics\": False,\n",
    "    \"loggers\": [\n",
    "       {   # Tensorboard logging\n",
    "           \"type\": 'tensorboard', \n",
    "           \"log_dir\": f\"../tensorboard/{folder_name}/log\",\n",
    "           \"log_every\": 4, \"log_images_every\": 60\n",
    "       }] \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56bc8794-d6bf-44cb-8d02-aaab32cb17eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Train\n",
    "\n",
    "trainer = Trainer.from_params(params)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6228a962-8a37-42e3-8e3e-1be275974799",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
